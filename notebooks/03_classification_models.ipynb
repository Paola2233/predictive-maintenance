{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286cfbec",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29148c",
   "metadata": {},
   "source": [
    "**Date:** July 25th, 2025  \n",
    "**Author:** Paola Rocha  \n",
    "**Description:** \n",
    "\n",
    "**Dataset:** [Microsoft Azure Predictive Maintenance](https://www.kaggle.com/datasets/arnabbiswas1/microsoft-azure-predictive-maintenance/data) on kaggle.\n",
    "\n",
    "**Content:**  \n",
    "* **Loading Data:** Importing the libraries and loading the datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860c007",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b82fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7dff8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 36600 entries, 0 to 36599\n",
      "Data columns (total 18 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   date                  36600 non-null  datetime64[ns]\n",
      " 1   machineID             36600 non-null  int64         \n",
      " 2   volt                  36600 non-null  float64       \n",
      " 3   rotate                36600 non-null  float64       \n",
      " 4   pressure              36600 non-null  float64       \n",
      " 5   vibration             36600 non-null  float64       \n",
      " 6   error_last_7_days     36600 non-null  float64       \n",
      " 7   error_last_14_days    36600 non-null  float64       \n",
      " 8   error_last_30_days    36600 non-null  float64       \n",
      " 9   failure_last_7_days   36600 non-null  float64       \n",
      " 10  failure_last_14_days  36600 non-null  float64       \n",
      " 11  failure_last_30_days  36600 non-null  float64       \n",
      " 12  maint_last_7_days     36600 non-null  float64       \n",
      " 13  maint_last_14_days    36600 non-null  float64       \n",
      " 14  maint_last_30_days    36600 non-null  float64       \n",
      " 15  will_fail_30_days     36600 non-null  float64       \n",
      " 16  model                 36600 non-null  int64         \n",
      " 17  age                   36600 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(14), int64(3)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "telemetry = pd.read_csv('../data/processed/telemetry.csv', parse_dates=['date']).sort_values(['machineID', 'date'])\n",
    "telemetry.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = telemetry.columns.drop(['date', 'will_fail_30_days'])\n",
    "target_col = 'will_fail_30_days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93c7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = telemetry[feature_cols]\n",
    "y = telemetry[target_col]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # don't shuffle time-series\n",
    ")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1181b87",
   "metadata": {},
   "source": [
    "## Model 1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of hyperparameters:  {'learning_rate': 0.01, 'max_depth': 3, 'max_leaves': 10, 'n_estimators': 100}\n",
      "Best score:  0.47035059331454965\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 6, 7],\n",
    "    'max_leaves': [10, 20, 30],\n",
    "    'learning_rate': [1, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d6dc7",
   "metadata": {},
   "source": [
    "## Experiment 2: Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ffcf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best set of hyperparameters:  {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Best score:  0.6003406374279007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:516: FitFailedWarning: \n",
      "240 fits failed out of a total of 960.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.56558282 0.56558282 0.56558282        nan 0.56558282\n",
      " 0.55642456 0.55642456        nan 0.56558282 0.56558282 0.56558282\n",
      "        nan 0.56558282 0.55642456 0.55642456        nan 0.43987194\n",
      " 0.42577604 0.43987194        nan 0.42577604 0.43987194 0.42577604\n",
      "        nan 0.42577604 0.42577604 0.43987194        nan 0.42577604\n",
      " 0.43987194 0.43987194        nan 0.49894521 0.48817    0.48817\n",
      "        nan 0.49903895 0.49903895 0.49894521        nan 0.48812557\n",
      " 0.49903895 0.49899452        nan 0.48812557 0.49875018 0.48812557\n",
      "        nan 0.42755746 0.44121038 0.42425908        nan 0.42615852\n",
      " 0.42615852 0.42534993        nan 0.42667884 0.438355   0.43823456\n",
      "        nan 0.44712349 0.43403969 0.43497003        nan 0.59629888\n",
      " 0.59629888 0.59629888        nan 0.59629888 0.59629888 0.60034064\n",
      "        nan 0.59629888 0.59629888 0.59629888        nan 0.60034064\n",
      " 0.60034064 0.60034064        nan 0.4333429  0.41834981 0.41834981\n",
      "        nan 0.41834981 0.4333429  0.4333429         nan 0.41834981\n",
      " 0.4333429  0.41834981        nan 0.41834981 0.41834981 0.41834981\n",
      "        nan 0.4680004  0.45816042 0.46809763        nan 0.45810756\n",
      " 0.46815149 0.46815149        nan 0.45821428 0.46810678 0.45816958\n",
      "        nan 0.46831843 0.4582206  0.46831843        nan 0.4420696\n",
      " 0.42759751 0.42754757        nan 0.43989991 0.42147878 0.44104853\n",
      "        nan 0.43557163 0.42502541 0.44090571        nan 0.44919781\n",
      " 0.44919781 0.43575653        nan 0.56558282 0.55642456 0.55642456\n",
      "        nan 0.55642456 0.55642456 0.55642456        nan 0.56558282\n",
      " 0.56558282 0.56558282        nan 0.55642456 0.55642456 0.55642456\n",
      "        nan 0.42577604 0.43987194 0.42577604        nan 0.42577604\n",
      " 0.42577604 0.42577604        nan 0.43987194 0.43987194 0.43987194\n",
      "        nan 0.42577604 0.43987194 0.43987194        nan 0.48806302\n",
      " 0.49894521 0.48806302        nan 0.49894521 0.49903895 0.48817\n",
      "        nan 0.49903895 0.49899452 0.48817           nan 0.48812557\n",
      " 0.49875018 0.49875018        nan 0.44121038 0.4411012  0.42642131\n",
      "        nan 0.42820703 0.4378849  0.42820703        nan 0.44337972\n",
      " 0.42358233 0.4266044         nan 0.45011618 0.44502135 0.45011618]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'criterion': [\"entropy\", \"gini\", \"log_loss\"],\n",
    "    'max_depth': [3, 5, 6, 7],\n",
    "    'min_samples_split': [1, 2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "model_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model_tree, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c2afb",
   "metadata": {},
   "source": [
    "## Model tracking performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0b072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\n",
    "        \"DecisionTreeClassifier\",\n",
    "        DecisionTreeClassifier(\n",
    "            criterion = 'gini',\n",
    "            max_depth = 3,\n",
    "            min_samples_leaf = 1,\n",
    "            min_samples_split = 5,\n",
    "            random_state = 42),\n",
    "        (X_train_scaled, y_train),\n",
    "        (X_test_scaled, y_test)\n",
    "    ),\n",
    "    (\n",
    "        \"XGBClassifier\",\n",
    "        xgb.XGBClassifier(\n",
    "            max_depth = 3,\n",
    "            max_leaves = 10,\n",
    "            n_estimators = 100,\n",
    "            learning_rate = 0.01,\n",
    "            random_state = 42\n",
    "        ),\n",
    "        (X_train_scaled, y_train),\n",
    "        (X_test_scaled, y_test)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e227b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.64      0.60      3218\n",
      "         1.0       0.68      0.61      0.64      4102\n",
      "\n",
      "    accuracy                           0.62      7320\n",
      "   macro avg       0.62      0.62      0.62      7320\n",
      "weighted avg       0.63      0.62      0.62      7320\n",
      "\n",
      "Model: XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.50      0.54      3218\n",
      "         1.0       0.65      0.72      0.68      4102\n",
      "\n",
      "    accuracy                           0.62      7320\n",
      "   macro avg       0.61      0.61      0.61      7320\n",
      "weighted avg       0.62      0.62      0.62      7320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for model_name, model, train_set, test_set in models:\n",
    "    X_train_scaled = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test_scaled = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca63604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/26 01:47:59 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/07/26 01:47:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "/Users/laurarocha/personal/predictive-maintenance/.venv/lib/python3.13/site-packages/xgboost/sklearn.py:1028: UserWarning: [01:47:59] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run DecisionTreeClassifier at: http://127.0.0.1:5000/#/experiments/581840100263335195/runs/529cc0d0d476485eb9de38ef8dad56ba\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/581840100263335195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/26 01:48:01 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/07/26 01:48:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBClassifier at: http://127.0.0.1:5000/#/experiments/581840100263335195/runs/64868dc8748d47b7af3c1f776a18d5e5\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/581840100263335195\n"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Machine Failure Prediction\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = reports[i]\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_metric('accuracy', report['accuracy'])\n",
    "        mlflow.log_metric('recall_class_1', report['1.0']['recall'])\n",
    "        mlflow.log_metric('recall_class_0', report['0.0']['recall'])\n",
    "        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])\n",
    "\n",
    "        if \"XGB\" in model_name:\n",
    "            mlflow.xgboost.log_model(model, name=\"model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, name=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b84bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
